import requests
import json
import time
import os

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    print("âš ï¸ è­¦å‘Š: æœªå®‰è£…openaiåº“ï¼Œæ–‡æœ¬ç”ŸæˆåŠŸèƒ½å°†ä¸å¯ç”¨")
    print("è¯·è¿è¡Œ: pip install openai")
    OPENAI_AVAILABLE = False
    OpenAI = None

def load_config():
    config_path = os.path.join(os.path.dirname(__file__), 'modelscope_config.json')
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return {
            "default_model": "Qwen/Qwen-Image",
            "timeout": 720,
            "image_download_timeout": 30,
            "default_prompt": "A beautiful landscape"
        }

def load_api_token():
    token_path = os.path.join(os.path.dirname(__file__), '.qwen_token')
    try:
        cfg = load_config()
        token_from_cfg = cfg.get("api_token", "").strip()
        if token_from_cfg:
            return token_from_cfg
    except Exception as e:
        print(f"è¯»å–config.jsonä¸­çš„tokenå¤±è´¥: {e}")
    try:
        if os.path.exists(token_path):
            with open(token_path, 'r', encoding='utf-8') as f:
                token = f.read().strip()
                return token if token else ""
        return ""
    except Exception as e:
        print(f"åŠ è½½tokenå¤±è´¥: {e}")
        return ""

def save_api_token(token):
    token_path = os.path.join(os.path.dirname(__file__), '.qwen_token')
    try:
        with open(token_path, 'w', encoding='utf-8') as f:
            f.write(token)
        cfg = load_config()
        cfg["api_token"] = token
        config_path = os.path.join(os.path.dirname(__file__), 'config.json')
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(cfg, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        print(f"ä¿å­˜tokenå¤±è´¥: {e}")
        return False

class QwenTextNode:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls):
        if not OPENAI_AVAILABLE:
            return {
                "required": {
                    "error_message": ("STRING", {
                        "default": "è¯·å…ˆå®‰è£…openaiåº“: pip install openai",
                        "multiline": True
                    }),
                }
            }
        config = load_config()
        saved_token = load_api_token()
        return {
            "required": {
                "user_prompt": ("STRING", {
                    "multiline": True,
                    "default": config.get("default_user_prompt", "ä½ å¥½")
                }),
                "api_token": ("STRING", {
                    "default": saved_token,
                    "placeholder": "è¯·è¾“å…¥æ‚¨çš„é­”æ­API Token"
                }),
            },
            "optional": {
                "system_prompt": ("STRING", {
                    "multiline": True,
                    "default": config.get("default_system_prompt", "You are a helpful assistant.")
                }),
                "model": ("STRING", {
                    "default": config.get("default_text_model", "Qwen/Qwen3-Coder-480B-A35B-Instruct")
                }),
                "max_tokens": ("INT", {
                    "default": 2000,
                    "min": 100,
                    "max": 8000
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1
                }),
                "stream": ("BOOLEAN", {
                    "default": True
                }),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("response",)
    FUNCTION = "generate_text"
    CATEGORY = "ModelScopeAPI"

    def generate_text(self, user_prompt="", api_token="", system_prompt="You are a helpful assistant.", model="Qwen/Qwen3-Coder-480B-A35B-Instruct", max_tokens=2000, temperature=0.7, stream=True, error_message=""):
        if not OPENAI_AVAILABLE:
            return ("è¯·å…ˆå®‰è£…openaiåº“: pip install openai",)
        
        config = load_config()
        
        if not api_token or api_token.strip() == "":
            raise Exception("è¯·è¾“å…¥æœ‰æ•ˆçš„API Token")
        
        saved_token = load_api_token()
        if api_token != saved_token:
            if save_api_token(api_token):
                print("âœ… API Tokenå·²è‡ªåŠ¨ä¿å­˜")
            else:
                print("âš ï¸ API Tokenä¿å­˜å¤±è´¥ï¼Œä½†ä¸å½±å“å½“å‰ä½¿ç”¨")
        
        try:
            print(f"ğŸ’¬ å¼€å§‹æ–‡æœ¬ç”Ÿæˆ...")
            print(f"ğŸ¤– æ¨¡å‹: {model}")
            print(f"ğŸ“ ç”¨æˆ·æç¤º: {user_prompt[:50]}...")
            print(f"âš™ï¸ ç³»ç»Ÿæç¤º: {system_prompt[:50]}...")
            print(f"ğŸŒ¡ï¸ æ¸©åº¦: {temperature}")
            print(f"ğŸ“Š æœ€å¤§tokens: {max_tokens}")
            print(f"âš¡ æµå¼è¾“å‡º: {stream}")
            
            client = OpenAI(
                base_url='https://api-inference.modelscope.cn/v1',
                api_key=api_token
            )
            
            messages = [
                {
                    'role': 'system',
                    'content': system_prompt
                },
                {
                    'role': 'user',
                    'content': user_prompt
                }
            ]
            
            print(f"ğŸš€ å‘é€APIè¯·æ±‚...")
            
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                stream=stream
            )
            
            if stream:
                print("ğŸ“¡ æ¥æ”¶æµå¼å“åº”...")
                full_response = ""
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        print(content, end='', flush=True)
                
                print(f"\nâœ… æµå¼ç”Ÿæˆå®Œæˆ!")
                print(f"ğŸ“„ æ€»é•¿åº¦: {len(full_response)} å­—ç¬¦")
                return (full_response,)
            else:
                result = response.choices[0].message.content
                print(f"âœ… æ–‡æœ¬ç”Ÿæˆå®Œæˆ!")
                print(f"ğŸ“„ ç»“æœé•¿åº¦: {len(result)} å­—ç¬¦")
                print(f"ğŸ“ ç»“æœé¢„è§ˆ: {result[:100]}...")
                return (result,)
            
        except Exception as e:
            error_msg = f"æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return (error_msg,)

if OPENAI_AVAILABLE:
NODE_CLASS_MAPPINGS = {
    "ModelScopeTextNode": ModelScopeTextNode
}
 
NODE_DISPLAY_NAME_MAPPINGS = {
    "ModelScopeTextNode": "ModelScope-Text æ–‡æœ¬ç”ŸæˆèŠ‚ç‚¹"
}
else:
    class OpenAINotInstalledNode:
        @classmethod
        def INPUT_TYPES(cls):
            return {
                "required": {
                    "install_command": ("STRING", {
                        "default": "pip install openai",
                        "multiline": False
                    }),
                }
            }
        
        RETURN_TYPES = ("STRING",)
        RETURN_NAMES = ("message",)
        FUNCTION = "show_install_message"
        CATEGORY = "ModelScopeAPI"
        
        def show_install_message(self, install_command):
            return ("è¯·å…ˆå®‰è£…openaiåº“æ‰èƒ½ä½¿ç”¨æ–‡æœ¬ç”ŸæˆåŠŸèƒ½: " + install_command,)
    
    NODE_CLASS_MAPPINGS = {
        "QwenTextNode": OpenAINotInstalledNode
    }

    NODE_DISPLAY_NAME_MAPPINGS = {
        "QwenTextNode": "Qwen-Text æ–‡æœ¬ç”ŸæˆèŠ‚ç‚¹ (éœ€è¦å®‰è£…openai)"
    }